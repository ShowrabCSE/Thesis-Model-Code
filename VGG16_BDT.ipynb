{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-zkerMKMpGg",
        "outputId": "08a3d76b-3966-4e7b-ca42-6c89e827a6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPvgC2ERvusD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnQ1Mvnp7jGE",
        "outputId": "f0859cee-cac7-4b9a-c17c-a6e4d60f762e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3251 files belonging to 3 classes.\n",
            "Found 416 files belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dir = \"/content/drive/MyDrive/Sayem_Potato_Leaf_/PlantVillage2/Training\"\n",
        "val_dar = \"/content/drive/MyDrive/Sayem_Potato_Leaf_/PlantVillage2/Validation\"\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  # validation_split=0.25,\n",
        "  # subset=\"training\",\n",
        "  # seed=123,\n",
        "  image_size=(128,128),\n",
        "  batch_size=64)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  val_dar,\n",
        "  # validation_split=0.25,\n",
        "  # subset=\"validation\",\n",
        "  # seed=123,\n",
        "  image_size=(128,128),\n",
        "  batch_size=64\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsHjev-YLFsl",
        "outputId": "d192914b-68ab-4cb6-e555-3a121e28eaa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "vgg = VGG16(input_shape = (128, 128, 3), weights = 'imagenet', include_top = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrz64brxLIhS",
        "outputId": "86874d04-b902-4076-aa90-091e454fb001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 24579     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14739267 (56.23 MB)\n",
            "Trainable params: 24579 (96.01 KB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg.output)\n",
        "prediction = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "model.summary()\n",
        "model.compile(\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  optimizer=\"adam\",\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the number of folds (k)\n",
        "k = 5\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "accuracy_scores = []\n",
        "loss_scores = []\n",
        "\n",
        "# Perform K-Fold Cross-Validation\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_ds)):\n",
        "    train_data = train_ds.take(train_index)\n",
        "    val_data = train_ds.take(val_index)\n",
        "\n",
        "    # Define checkpoint callback to save the best model\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/Road_To_BCS/vgg16.h5\" #checkpoint path ba model save er path deya lagbe\n",
        "\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model with checkpoint callback\n",
        "    history = model.fit(train_data, validation_data=val_data, epochs=15, callbacks=[checkpoint], shuffle=True)\n",
        "\n",
        "    # Evaluate the best model on the validation data\n",
        "    best_model = tf.keras.models.load_model(checkpoint_path)\n",
        "    loss, accuracy = best_model.evaluate(val_data)\n",
        "\n",
        "    # Append evaluation metrics to the lists\n",
        "    loss_scores.append(loss)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print average evaluation metrics\n",
        "print(\"Average Loss:\", np.mean(loss_scores))\n",
        "print(\"Average Accuracy:\", np.mean(accuracy_scores))"
      ],
      "metadata": {
        "id": "XARZ8LqMo4Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the number of folds (k)\n",
        "k = 5\n",
        "\n",
        "# Initialize KFold cross-validation\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initialize lists to store evaluation metrics\n",
        "accuracy_scores = []\n",
        "loss_scores = []\n",
        "\n",
        "# Convert datasets to arrays for compatibility with KFold.split\n",
        "train_ds_array = np.array(list(train_ds.as_numpy_iterator()))\n",
        "val_ds_array = np.array(list(val_ds.as_numpy_iterator()))\n",
        "\n",
        "# Perform K-Fold Cross-Validation\n",
        "for train_index, val_index in kf.split(train_ds_array):\n",
        "    train_data = train_ds_array[train_index]\n",
        "    val_data = train_ds_array[val_index]\n",
        "\n",
        "    # Define checkpoint callback to save the best model\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/Road_To_BCS/vgg16.h5\"\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "    # Train the model with checkpoint callback\n",
        "    history = model.fit(train_data[0], train_data[1], validation_data=(val_data[0], val_data[1]), epochs=15, callbacks=[checkpoint], shuffle=True)\n",
        "\n",
        "    # Evaluate the best model on the validation data\n",
        "    best_model = tf.keras.models.load_model(checkpoint_path)\n",
        "    loss, accuracy = best_model.evaluate(val_data[0], val_data[1])\n",
        "\n",
        "    # Append evaluation metrics to the lists\n",
        "    loss_scores.append(loss)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print average evaluation metrics\n",
        "print(\"Average Loss:\", np.mean(loss_scores))\n",
        "print(\"Average Accuracy:\", np.mean(accuracy_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "N-pH9uCLqy6k",
        "outputId": "12617523-f7f0-427d-d305-7c1a4bd43a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (51, 2) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-967214cf20ab>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Convert datasets to arrays for compatibility with KFold.split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_ds_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mval_ds_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (51, 2) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzS25tXLLSFZ"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, validation_data = val_ds, epochs = 15, batch_size = 64,\n",
        "                    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHw9NmvmKpad"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'validation_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.1, 1.2])\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlcYUCcCH1uW"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label = 'validation_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 6])\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsApLiOKH4EH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class_names = ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']\n",
        "\n",
        "all_val_labels = []\n",
        "all_val_predictions = []\n",
        "\n",
        "# Evaluate the model on the validation dataset batch by batch\n",
        "for batch in val_ds:\n",
        "    val_images, val_labels_batch = batch\n",
        "    val_predictions_batch = model.predict(val_images)\n",
        "\n",
        "    val_labels_batch = np.array(val_labels_batch)\n",
        "    val_predictions_batch = np.argmax(val_predictions_batch, axis=-1)\n",
        "\n",
        "    all_val_labels.extend(val_labels_batch)\n",
        "    all_val_predictions.extend(val_predictions_batch)\n",
        "\n",
        "# Calculate the confusion matrix for all validation data\n",
        "conf_matrix = confusion_matrix(all_val_labels, all_val_predictions)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilknPhxEIBld"
      },
      "outputs": [],
      "source": [
        "print(classification_report(all_val_labels, all_val_predictions, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}